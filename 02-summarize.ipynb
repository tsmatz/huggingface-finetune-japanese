{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14410483",
   "metadata": {},
   "source": [
    "# Hugging Face - Summarization in Japanese\n",
    "\n",
    "This source code builds the fine-tuned model of [google/mt5-small](https://huggingface.co/google/mt5-small) for Japanese summarization.\n",
    "\n",
    "For more background and details, see [this blog post](https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/).\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/huggingface-finetune-japanese/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4dd94",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "In order to install core components, see [Readme](https://github.com/tsmatz/huggingface-finetune-japanese/).<br>\n",
    "Install additional packages for running this notebook as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bf825",
   "metadata": {},
   "source": [
    "Install packages depending on T5 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a86426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b66d94",
   "metadata": {},
   "source": [
    "Install packages depending on rouge evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee740d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install absl-py rouge_score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a81ff",
   "metadata": {},
   "source": [
    "Install other dependent packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95588a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ef40e",
   "metadata": {},
   "source": [
    "## Check device\n",
    "\n",
    "Check whether GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bc098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled.\n",
      "device count: 1, current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is enabled.\")\n",
    "    print(\"device count: {}, current device: {}\".format(torch.cuda.device_count(), torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"GPU is not enabled.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163ab77",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "In this example, we use [XL-Sum Japanese dataset](https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/japanese) in Hugging Face, which is the annotated article-summary pairs generated by BBC.<br>\n",
    "This dataset has around 7000 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a1866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5d77472b98489b92504d4e34e804f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb0f6768117431c82b033daa2dbeb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xlsum/japanese to /home/tsmatsuz/.cache/huggingface/datasets/csebuetnlp___xlsum/japanese/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa7bfec30c4c39a7d7d1578267477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xlsum downloaded and prepared to /home/tsmatsuz/.cache/huggingface/datasets/csebuetnlp___xlsum/japanese/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a499e3e8c44a3c88245235dd6f0f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 7113\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csebuetnlp/xlsum\", name=\"japanese\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8da181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '44789754',\n",
       " 'url': 'https://www.bbc.com/japanese/44789754',\n",
       " 'title': 'タイ洞窟から少年とコーチ、全員無事救出',\n",
       " 'summary': 'タイ北部のタムルアン洞窟で10日夜、中に閉じ込められていた少年12人とサッカー・コーチの計13人のうち、最後の少年4人とコーチが水路を潜り無事脱出した。その約3時間後には、洞窟内で少年たちと留まっていた海軍ダイバー3人と医師も生還した。17日間も洞窟内にいた13人の救出に、タイ国内外で多くの人が安心し、喜んでいる。',\n",
       " 'text': '救出作戦の間、洞窟内に少年たちと留まったタイ海軍のダイバーと医師も最後に無事脱出した。4人の写真は10日、タイ海軍特殊部隊がフェイスブックに掲載したもの タイ海軍特殊部隊はフェイスブックで、「これは奇跡なのか科学なのか、一体何なのかよくわからない。『イノシシ』13人は全員、洞窟から出た」と救助作戦の終了を報告した。「イノシシ」（タイ語で「ムーパ」）は少年たちの所属するサッカー・チームの愛称。 遠足に出かけた11歳から17歳の少年たちと25歳のサッカー・コーチは6月23日、大雨で増水した洞窟から出られなくなった。タイ内外から集まったダイバー約90人などが捜索に当たり、英国人ダイバー2人によって7月2日夜に発見された。地元のチェンライ県知事やタイ海軍特殊部隊が中心となった救助本部は当初、水が引くか、あるいは少年たちが潜水技術を習得するまで時間をかけて脱出させるつもりだったが、雨季による水位上昇と洞窟内の酸素低下の進行が懸念され、8日から3日連続の救出作戦が敢行された。 少年たちの脱出方法 ダイバーたちに前後を支えられ、水路内に張り巡らされたガイドロープをたどりながら、潜水経験のない少年たちは脱出した。8日に最初の4人、9日に4人、10日に残る5人が脱出し、ただちに近くのチェンライ市内の病院に搬送された。2週間以上洞窟に閉じ込められていたことを思えば、全員驚くほど心身ともに元気だという。 少年たちとコーチはレントゲンや血液検査などを受けた。少なくとも7日間は、経過観察のために入院を続けるという。 洞窟内の水を飲み、鳥やコウモリの排泄物に接触した可能性のある13人は、病原体に感染しているおそれがあるため隔離されている。家族とはガラス越しに再会したという。 食べ物のほとんどない洞窟内で2週間以上を過ごした少年たちは体重を大幅に落とし、空腹を訴えていた。救出後は好物の豚肉のご飯やパン、チョコレートなどを希望したが、しばらくは流動食が続くという。 さらに、外界の光に目が慣れるまでの数日は、サングラスをかける必要がある。 ＜おすすめ記事＞ 救出作戦が終わると、洞窟の出口に集まった救助関係者から大きな歓声が上がった。山のふもとには、少年たちが所属する「ムーパ（イノシシ）」サッカーチームの関係者の家があり、そこに集まった人たちも笑顔で叫んだり歓声を挙げたりした。現場にいたBBCのジョナサン・ヘッド記者は、喜ぶ人たちは「とてもタイ人らしくない様子で」さかんに握手をして回っていたと伝えた。 少年たちへの精神的影響は？ タイ洞窟救助 チェンライ市では、全員脱出の知らせに往来の車は次々にクラクションを鳴らして喜んだ。子供たちやコーチが搬送された病院の外に集まっていた人たちは、一斉に拍手した。 ソーシャルメディアではタイ人の多くが、「#Heroes(英雄）」、「 #Thankyou（ありがとう）」などのハッシュタグを使って、それぞれに思いを表現していた。 13人は2日、洞窟内の岩場に身を寄せているところを発見された。中央の少年は、サッカーのイングランド代表のシャツを着ている。写真はタイ海軍が4日に公表したビデオより サッカー界も少年たちとコーチの無事を大いに喜び、英マンチェスター・ユナイテッドやポルトガルのベンフィカが全員を試合に招待した。国際サッカー連盟（FIFA）も、少年たちをロシアで開催されているワールドカップの15日にある決勝戦に招いたが、これは回復が間に合わないという理由で見送られた。 ワールドカップの準決勝に備えるイングランド代表のDFカイル・ウォーカーは、イングランドのユニフォームを少年たちに贈りたいとツイートした。少年の1人は洞窟内で、イングランドのジャージーを着ていた。すると英外務省の公式アカウントがこれに応えて、「やあ、カイル。駐タイ英国大使と話をした。イングランドのシャツを勇敢な少年たちに、喜んで、確実に届けてくれるそうだ」とツイートした。 経験豊富なダイバーにとっても、少年たちのいる場所までの往復は重労働だった。元タイ海軍潜水士のサマン・グナンさんは6日、少年たちに空気ボンベを運ぶ任務を果たして戻ろうとしていたところ、酸素不足で命を落とした。 ダイバーたちが出口まで張ったガイドロープをたどりながら、少年たちは場所によって、歩いたり、水の中を歩いたり、登ったり潜ったりして外に出た。 少年たちは、通常のマスクよりも初心者に適した顔部全体を覆うマスクをかぶった。少年1人につき2人のダイバーが付き、ダイバーが少年の空気ボンベを運んだ。 最も困難なのは、洞窟の中ほどにある「Tジャンクション」と呼ばれている場所で、あまりに狭いため、ダイバーは空気ボンベを外して進む必要があった。 Tジャンクションを抜けると、ダイバー達の基地となっている「第3室」があり、少年たちはここで出口へ向かう前に休息がとれた。 少年らの救出経路。下方の赤い丸が少年たちの見つかった場所。人の形が実際の人間の身長。青い部分は潜水しないと進めない。高さが1メートルに満たない箇所もある。トンネル内で最も狭い部分は、人1人がやっと通れるぐらいのスペースしかない。上方の白い部分は、ところどころ浅い水があるが、ほとんどが乾いた岩場 （英語記事 Cave rescue: Elation as Thai boys and coach freed by divers）'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c922d",
   "metadata": {},
   "source": [
    "To generate inputs for fine-tuning, now I tokenize each text and convert into token ids.\n",
    "\n",
    "First, load tokenizer in pre-trained ```google/mt5-small``` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42045d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a47ce560f104e97a9bdefa4b663fa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1389bb54a5a44c2815bc0dafe1ad7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e159be6297f40a69847007ea9adde9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c94704d8274a09bb4aa8a1f684b168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8231f7",
   "metadata": {},
   "source": [
    "For fine-tuning, apply tokenization for dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714d1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample_data(data):\n",
    "    # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "    # Here I restrict these token size.\n",
    "    input_feature = t5_tokenizer(data[\"text\"], truncation=True, max_length=1024)\n",
    "    label = t5_tokenizer(data[\"summary\"], truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": input_feature[\"input_ids\"],\n",
    "        \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "        \"labels\": label[\"input_ids\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b32615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea90c74e3c4eeaa8246b96b13f70d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae3153987534235a8c93aa2e81b6b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb75ad7742c49a3a46bcef76eeb2acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7113\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(\n",
    "    tokenize_sample_data,\n",
    "    remove_columns=[\"id\", \"url\", \"title\", \"summary\", \"text\"],\n",
    "    batched=True,\n",
    "    batch_size=128)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dd056",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf379a1",
   "metadata": {},
   "source": [
    "In this example, we use mT5 model.<br>\n",
    "There exist several sizes of mT5 and I'll use small one (```google/mt5-small```) to fit to memory in my machine. The name is \"small\", but it's still so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cab1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18975c2c1d4c403bbfa1da11837ed05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "# see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "mt5_config = AutoConfig.from_pretrained(\n",
    "    \"google/mt5-small\",\n",
    "    max_length=128,\n",
    "    length_penalty=0.6,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=15,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb5336",
   "metadata": {},
   "source": [
    "We prepare data collator, which works for preprocessing data.\n",
    "\n",
    "For the sequence-to-sequence (seq2seq) task, we need to not only stack the inputs for encoder, but also prepare for the decoder side. In seq2seq setup, a common technique called \"teach forcing\" will then be applied in decoder.<br>\n",
    "These tasks are not needed to manually setup in Hugging Face, and ```DataCollatorForSeq2Seq``` will take care of all steps.\n",
    "\n",
    "In this collator, the padded token will also be filled with label id -100.<br>\n",
    "This token will then be ignored in the sebsequent loss computation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc213f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    t5_tokenizer,\n",
    "    model=model,\n",
    "    return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c9f14",
   "metadata": {},
   "source": [
    "We also prepare metrics function for evaluation in the training.<br>\n",
    "Measuring the quality of generated text is very difficult, and BLEU and ROUGE are often used.\n",
    "\n",
    "Briefly speaking, BLEU measures how many of n-grams in the generated (predicted) text are overlaped in the reference text. This score is used for evaluation, especially in the machine translation task.\n",
    "However, in summarization, we need all important words (which appears on the reference text) in the generated text. This is because we often use ROUGE in summarization tasks.\n",
    "The idea of ROUGE is similar to BLEU, but it also measures how many of n-grams in the reference text appears in the generated (predicted) text. (This is why the name of ROUGE includes \"RO\", which means \"Recall-Oriented\".)<br>\n",
    "There also exist variations, ROUGE-L and ROUGE-Lsum, which also measures the longest common substrings (LCS).\n",
    "\n",
    "In Hugging Face, you don't need to manually implement these logics and can use built-in objects for scoring these matrics.<br>\n",
    "In this example, I have configured mT5 tokenization as custom tokenization in computation (which is based on SentencePiece Unigram segmentation), because the white space tokenization is used as default in ROUGE evaluation.\n",
    "\n",
    "> Note : You can also specify multilingual stemmer.\n",
    "\n",
    "> Note : As I have mentioned above, the padded token id becomes -100 by data collator and I then also convert it into padded token id before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5dd0da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11360dd3bec49c299c6caebca08f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg = t5_tokenizer(arg)\n",
    "    return t5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Add punctuation before sentence tokenization\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde59f4",
   "metadata": {},
   "source": [
    "Before fine-tuning, now I check ROUGE score with plain mT5 model. Here I check scores for top 5 rows in test dataset.\n",
    "\n",
    "The score is very low, because this model is not trained for any downstream tasks. (It's just trained by unsupervised approach.)\n",
    "\n",
    "> Note : In order to avoid suboptimal text generation, here I have applied beam search for the text generation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e99037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.06507595140684969,\n",
       " 'rouge2': 0.03374507990060038,\n",
       " 'rougeL': 0.0650759514068497,\n",
       " 'rougeLsum': 0.06448736750547615}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "    tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "    with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            num_beams=15,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=1,\n",
    "            remove_invalid_values=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "    break\n",
    "\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bfded",
   "metadata": {},
   "source": [
    "We prepare training arguments for fine-tuning.<br>\n",
    "In this example, we use HuggingFace transformer trainer class, with which you can run training without manually writing training loop.\n",
    "\n",
    "In usual training evaluation, training loss and accuracy will be computed and evaluated, by comparing the generated logits with labels. However, as we saw above, we want to evaluate ROUGE score using the predicted tokens.<br>\n",
    "To simplify these sequence-to-sequence specific steps, here I use built-in ```Seq2SeqTrainingArguments``` and ```Seq2SeqTrainer``` classes in HuggingFace, instead of usual ```TrainingArguments``` and ```Trainer```.<br>\n",
    "By setting ```predict_with_generate=True``` in this class, the predicted tokens generated by  ```model.generate()``` will be used in each evaluation.\n",
    "\n",
    "The checkpoint files (in each 500 steps) are saved in the folder named ```mt5-summarize-ja```.\n",
    "\n",
    "> Note : Do not use FP16 precision in mT5 fine-tuning.\n",
    "\n",
    "> Note : In general, the saved checkpoints in the training will become so large.<br>\n",
    "> Set ```save_total_limit``` property (which limits the total amount of checkpoints by deleting the older ones) to save disk spaces, or expand disks in Azure VM. (See [here](https://learn.microsoft.com/en-us/azure/virtual-machines/linux/expand-disks) to expand disks in Azure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f28731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"mt5-summarize-ja\",\n",
    "    log_level = \"error\",\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-4,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    warmup_steps = 90,\n",
    "    optim = \"adafactor\",\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length = 128,\n",
    "    save_steps = 500,\n",
    "    logging_steps = 10,\n",
    "    push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978f071",
   "metadata": {},
   "source": [
    "Build trainer. (Put it all together.)\n",
    "\n",
    "Because the cost of evaluation computation (ROUGE scoring) is so high, I have then decreased the number of rows in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca8a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = metrics_func,\n",
    "    train_dataset = tokenized_ds[\"train\"],\n",
    "    eval_dataset = tokenized_ds[\"validation\"].shard(num_shards=45, index=0),\n",
    "    tokenizer = t5_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b530f",
   "metadata": {},
   "source": [
    "Now let's run training.<br>\n",
    "As you will find, ROUGE scores are growing during training.\n",
    "\n",
    "> Note : As I have mentioned above, make sure that you have enough disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66059ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2220' max='2220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2220/2220 5:10:00, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.213300</td>\n",
       "      <td>3.428347</td>\n",
       "      <td>0.280613</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0.242478</td>\n",
       "      <td>0.249972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.640700</td>\n",
       "      <td>3.059172</td>\n",
       "      <td>0.330911</td>\n",
       "      <td>0.172586</td>\n",
       "      <td>0.283457</td>\n",
       "      <td>0.291307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.352300</td>\n",
       "      <td>2.855823</td>\n",
       "      <td>0.315189</td>\n",
       "      <td>0.159415</td>\n",
       "      <td>0.273967</td>\n",
       "      <td>0.284733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.294700</td>\n",
       "      <td>2.776553</td>\n",
       "      <td>0.371320</td>\n",
       "      <td>0.208282</td>\n",
       "      <td>0.306402</td>\n",
       "      <td>0.323169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.156700</td>\n",
       "      <td>2.724403</td>\n",
       "      <td>0.367068</td>\n",
       "      <td>0.195358</td>\n",
       "      <td>0.311227</td>\n",
       "      <td>0.319018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.059600</td>\n",
       "      <td>2.652614</td>\n",
       "      <td>0.390743</td>\n",
       "      <td>0.226202</td>\n",
       "      <td>0.334307</td>\n",
       "      <td>0.344931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.826800</td>\n",
       "      <td>2.645617</td>\n",
       "      <td>0.408129</td>\n",
       "      <td>0.235066</td>\n",
       "      <td>0.346507</td>\n",
       "      <td>0.362483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.924800</td>\n",
       "      <td>2.579337</td>\n",
       "      <td>0.393283</td>\n",
       "      <td>0.213991</td>\n",
       "      <td>0.321050</td>\n",
       "      <td>0.337933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.761400</td>\n",
       "      <td>2.572187</td>\n",
       "      <td>0.391245</td>\n",
       "      <td>0.219717</td>\n",
       "      <td>0.323080</td>\n",
       "      <td>0.336895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.754600</td>\n",
       "      <td>2.532084</td>\n",
       "      <td>0.400207</td>\n",
       "      <td>0.226026</td>\n",
       "      <td>0.339965</td>\n",
       "      <td>0.356957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.672100</td>\n",
       "      <td>2.534742</td>\n",
       "      <td>0.393562</td>\n",
       "      <td>0.222604</td>\n",
       "      <td>0.316367</td>\n",
       "      <td>0.336393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.584600</td>\n",
       "      <td>2.512818</td>\n",
       "      <td>0.406965</td>\n",
       "      <td>0.234266</td>\n",
       "      <td>0.340601</td>\n",
       "      <td>0.358809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.699100</td>\n",
       "      <td>2.486371</td>\n",
       "      <td>0.426076</td>\n",
       "      <td>0.251046</td>\n",
       "      <td>0.352634</td>\n",
       "      <td>0.370274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.580400</td>\n",
       "      <td>2.482601</td>\n",
       "      <td>0.420317</td>\n",
       "      <td>0.241729</td>\n",
       "      <td>0.339656</td>\n",
       "      <td>0.355914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.541000</td>\n",
       "      <td>2.455600</td>\n",
       "      <td>0.414119</td>\n",
       "      <td>0.237253</td>\n",
       "      <td>0.338759</td>\n",
       "      <td>0.361529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.542700</td>\n",
       "      <td>2.478829</td>\n",
       "      <td>0.420654</td>\n",
       "      <td>0.245518</td>\n",
       "      <td>0.339049</td>\n",
       "      <td>0.356091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.521300</td>\n",
       "      <td>2.479011</td>\n",
       "      <td>0.405923</td>\n",
       "      <td>0.231522</td>\n",
       "      <td>0.336428</td>\n",
       "      <td>0.356382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.510400</td>\n",
       "      <td>2.469027</td>\n",
       "      <td>0.401383</td>\n",
       "      <td>0.221386</td>\n",
       "      <td>0.331325</td>\n",
       "      <td>0.345706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.448400</td>\n",
       "      <td>2.452747</td>\n",
       "      <td>0.421265</td>\n",
       "      <td>0.247799</td>\n",
       "      <td>0.348018</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.480200</td>\n",
       "      <td>2.454749</td>\n",
       "      <td>0.425816</td>\n",
       "      <td>0.251049</td>\n",
       "      <td>0.352994</td>\n",
       "      <td>0.374027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.486400</td>\n",
       "      <td>2.459233</td>\n",
       "      <td>0.425010</td>\n",
       "      <td>0.241521</td>\n",
       "      <td>0.348273</td>\n",
       "      <td>0.361852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.394600</td>\n",
       "      <td>2.457034</td>\n",
       "      <td>0.427435</td>\n",
       "      <td>0.253420</td>\n",
       "      <td>0.351782</td>\n",
       "      <td>0.370063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2220, training_loss=3.0295667949023546, metrics={'train_runtime': 18608.4195, 'train_samples_per_second': 3.822, 'train_steps_per_second': 0.119, 'total_flos': 6.575542928726016e+16, 'train_loss': 3.0295667949023546, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44578fb6",
   "metadata": {},
   "source": [
    "In order to use it later, you can save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0710012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./trained_for_summarization_jp\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "    trainer.model.module.save_pretrained(\"./trained_for_summarization_jp\")\n",
    "else:\n",
    "    trainer.model.save_pretrained(\"./trained_for_summarization_jp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2efe66",
   "metadata": {},
   "source": [
    "Load pre-trained model from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be7c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"./trained_for_summarization_jp\")\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6f566",
   "metadata": {},
   "source": [
    "## Generate Text (Summarize) with Fine-Tuned Model\n",
    "\n",
    "Now let's see how it generates text for summarization with fine-tuned model.<br>\n",
    "Here I generate the summarized text of test data, which has not seen in the training set.\n",
    "\n",
    "> Note : The article in XL-Sum dataset is created by removing the first sentence (headline sentence) of BBC news source, and the first sentence is then used for summary.<br>\n",
    ">  For this reason, there might exist several mismatch between article and summary in test data. (Choose appropriate samples for checking.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca11263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "トム・エッジントン BBCリアリティー・チェック（ファクトチェック）チーム かつて労働党党首も務めたブレア氏は、BBCラジオ4の番組「Today」で、「議会は行き詰まった。議会が決められないなら、国民が決める形に戻ろう」と語った。 労働党の公式な立場は、テリーザ・メイ英首相が欧州連合（EU）と合意した離脱協定案が議会で否決された場合、解散総選挙の圧力をかけるというもの。もし総選挙が実現しなかった場合は、再度の国民投票を支持するのも選択肢になりうると、労働党は表明している。 しかしメイ首相は、再国民投票の予測を否定している。メイ氏は下院議員らに対し、2016年に実施した国民投票の結果が「尊重されるべきだ」と繰り返し語ってきた。 だが、もしブレア氏が求めている通り、下院がブレグジットをめぐる膠着（こうちゃく）状態を打ち破るために2度目の国民投票を実施すると決定したら、どうなるのだろうか？ 英選挙管理委員会はBBCニュースに対し、「適切な対応策」を有しており、「あらゆる予定外の投票に迅速に対応する」準備ができていると語った。 期限は迫っている イギリスのEU離脱予定日は、2019年3月29日。残り100日を切り、時間が最も差し迫った問題だ。 英議会が2度目の国民投票実施を採択した場合、投票規則や選挙運動規則を定める法律にも上下両院の支持が必要になる。 2016年の国民投票では、投票日の7カ月前に関連法案が議決された。 しかし、今回はもっと早い法制化が可能なのだろうか？ 法制化の速度を上げるため、前回の国民投票に関する諸規則を定めた2015年国民投票法をひな型にし、実質的に大部分を写してしまうのが、あり得る選択肢の1つだ。 英ユニヴァーシティー・コレッジ・ロンドン公共政策大学院憲法ユニットのアラン・レンウィック副ユニット長は、「理論上、このやり方は非常に素早く完了できる」と話す。 もしこのやり方が採用されても、法案の議会通過はおよそ11週間かかるとレンウィック氏は推計している。 この予定表を基にすると、法案通過は2月後半になると予想される。ただし、法制過程を今開始すればの話だ。 投票用紙の選択肢を、2016年の国民投票における「離脱」か「残留」かの2択ではなくし、複数の選択肢を含めるよう下院が要求した場合、かかる時間はもっとずっと長くなると、レンウィック氏は付け加える。 2016年の国民投票でEU残留派として活動したトニー・ブレア元首相は、2度目の国民投票が議会の膠着状態を解消する可能性があると主張する 何を問うのか あり得る選択肢の範囲からどんな質問を選ぶかは、最終的には英議会の決定に委ねられる。 離脱協定の最終合意に国民投票を求める「People's Vote （人民の投票）」運動の見解では、テリーザ・メイ首相の離脱協定とEU残留のどちらかを選ぶのが推奨される選択肢だが、投票参加者に3つの選択肢から選ばせる可能性も除外しないという。 再び国民投票が実施されるなら、投票用紙に「残留」の選択肢はあるべきでなく、メイ首相の離脱協定か、合意なしでのEU離脱かの二者択一であるべきとの主張もある。 他の選択肢もある。デイヴィッド・キャメロン内閣で運輸相や国際開発相、メイ内閣で教育相や女性・平等担当相を歴任し、2度目の国民投票を支持しているジャスティーン・グリーニング氏は以前、3つの選択肢を求めた――。 複数の選択肢がある場合、下院はどんな投票制度を使うかも決定する必要がある。たとえば、選択肢を1つ選ぶのか、望ましいほうから順番をつけるのか、というように。 選挙管理委員会は、提案された質問を試験し、それらが「明白に、単純に、そして中立に」示されていると確認する必要もある。 選挙運動を行う公認団体の選定も必要だ。 選挙管理委員会はそれから、国民投票の参加方法を有権者に情報提供する必要がある。また、全国で開票担当者の確保も必要だ。 これらの準備が終わると、選挙運動期間が始まる。運動期間は、通常4週間続く。そしてやっと、投票そのものが実施される。 ジャスティーン・グリーニング氏は2度目の国民投票案を支持し、国民には3つの選択肢が与えられるべきだと主張する 選管はBBCニュースに対し、2000年制定の政党、選挙、国民投票法で定められている法案通過から投票当日までの全ての手順には、最短でも10週間かかると説明した。 このことから、法案通過と選挙過程の両方が、イギリスがEUを離脱する予定期日である2019年3月29日までに終わる可能性は極めて低いと示唆される。 発表から10日での国民投票 しかし、非常に厳しい時期内に国民投票を実施した前例が他国にないわけではない。 3年前、ギリシャは1週間ほどの準備期間で国民投票をとりまとめた。有権者はこの国民投票で、同国の経済危機に対する国際債権団の救済案を否決した。 しかし、国民投票をあまりに早急に実施してしまうと、「通常の手続きに従っていない」との印象を与え、有権者が最終結果を非合法なものとみてしまう可能性があると、レンウィック氏は語る。 たとえば、2015年のギリシャと似た準備期間で国民投票をすると、郵送による投票を受け付けたり、投票用紙に書かれる質問を評価したりする十分な時間の確保が許されないことになる。 リスボン条約50条で定められた期間の延長 イギリスはEUに対し、EU基本条約（リスボン条約）第50条で定められた離脱交渉期間を延長するよう求める可能性もある。リスボン条約は、メイ首相が第50条を発動した2017年3月29日から2年間を、離脱条件の合意に必要な期間として定めている。この期間が延長されれば、新たな国民投票を実施するための時間が増える。 しかし、英ケンブリッジ大学で欧州法を研究するキャスリン・バーナード教授によると、2019年3月29日という離脱期限を延長するあらゆる試案には、他のEU加盟27カ国の全会一致での支持が必要になる。 EUは、イギリスの離脱延期を認める可能性があると示唆している。ただし、たとえば総選挙もしくは新たな国民投票が実施されるなど、政局が変化した場合のみだという。すでに合意された離脱協定について、単に再交渉するための追加時間の確保は認められない。 また、期間延長にはイギリス議会の同意も必要になる。 ＜関連記事＞ さらに欧州司法裁判所（ECJ）は先に、イギリスは他国の承認を得ずにリスボン条約第50条の発動を完全に取り消す権限を持つとの判断を下した。 しかしこれは、ブレグジットの過程全体を中止できるという意味だ。単に延期するという意味ではない。 なので結局、イギリスが2度目の国民投票実施を望むなら、まずは第50条で定められた離脱期間の延長を模索することになるだろう。 そして、投票の結果に従って、イギリスは投票後に第50条の発動を撤回するかどうか決められる。 イギリスのEU離脱予定日後に国民投票を実施するのも代替案かもしれない。しかし、この案はかなり現実的な困難を引き起こしかねない。特に、既に離脱したにもかかわらず、イギリス国民がEUの一員であり続ける選択をした場合には。 （英語記事 Brexit: How could another referendum on leaving the EU work?）\n",
      "***** Summary Text (True Value) *****\n",
      "ブレグジット(イギリスの欧州連合離脱)をめぐり、イギリスのトニー・ブレア元首相が、2度目の国民投票を求める主張を再び繰り広げている。可能性があるブレグジットの形態1つ1つについて議会で投票し、どれも合意に達しない場合に実施すべきという。\n",
      "***** Summary Text (Generated Text) *****\n",
      "テリーザ・メイ英首相が欧州連合(EU)離脱をめぐる膠着状態を打ち破るために2度目の国民投票を実施すると決定した。\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "    tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "    with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            num_beams=15,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=1,\n",
    "            remove_invalid_values=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "    break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(ds[\"test\"][\"text\"][0])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[0])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4da2045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "このワクチンは複数の動物実験で、安全性や、効果的な免疫反応を引き起こすことが示されている。 今回の第1段階の後には、6000人を対象とした別の臨床試験が今年10月に予定されている。 インペリアル･コレッジ･ロンドンのチームは、2021年の早い時期からイギリスや海外でワクチンを配布できるようにしたいとしている。 ＜関連記事＞ 世界中では約120のワクチンの開発が進められている。英オックスフォード大学の専門家たちはすでに臨床試験を開始している。 新しいアプローチ 多くの従来のワクチンは、弱体化させたウイルスや改変したウイルスなどがもとになっている。しかし今回のワクチンは新しいアプローチに基づいたもので、遺伝子のRNA（リボ核酸）を使う。 筋肉に注射すると、RNAは自己増殖し、新型ウイルスの表面にみられるスパイクタンパク質のコピーをつくるよう、体内の細胞に指示を出す。 この方法で、COVID-19（新型ウイルスによる感染症）を発症することなく新型ウイルスを認識して戦うための免疫システムを訓練できるという。 シャトック教授は、「我々はゼロからワクチンを製造し、わずか数カ月で臨床試験に持ち込むことができた」と述べた。 「我々のアプローチがうまくいって、ワクチンがこの病気を効果的に防御できれば、将来的なアウトブレイク（大流行）への対応方法に革命をもたらす可能性がある」 主任研究員のカトリーナ・ポロック博士は、ワクチンの効果に期待している この研究の主任研究員、カトリーナ・ポロック博士は、「参加者に大きな免疫反応がみられるだろうと、慎重ながらも楽観的に感じられなかったら、私はこの臨床試験に取り組んでいなかっただろう」と付け加えた。 「前臨床データは非常に期待がもてるものだった。感染から保護しておきたい免疫反応である中和抗体応答は確認できているが、このワクチンを評価するにはまだ道のりは長い」 この研究は英政府から4100万ポンド（約54億5500万円）の資金提供を受けている。ほかにも500万ポンド（約6億6500万円）の寄付が寄せられている。 「ウイルスを倒すのに協力したくて志願」 金融業界で働くキャシーさん（39）は、インペリアル･コレッジ･ロンドンの臨床試験に参加している最初のボランティアの1人だ。 新型ウイルスとの戦いの一端を担いたくて志願したという。 「自分に何ができるのかあまりよく分かっていなかったけど、これが私にできることだったと分かった」 「それに、ワクチンができるまで日常に戻れる可能性は低いことを理解したことで、ワクチン開発の一端を担いたいと思った」 キャシーさんは、インペリアル･コレッジ･ロンドンの臨床試験に参加している最初のボランティア300人の1人 こうした中、ケンブリッジ公爵ウィリアム王子はオックスフォード大学の臨床試験に参加しているボランティアたちと、オックスフォード市内のチャーチル病院で面会した。 ウィリアム王子はボランティアに対し、「みなさん全員が参加しているのは、信じられないくらい胸が躍る、非常に待ち望まれたプロジェクトだ。だからみんなが心を奪われている」と述べた。 初日の被験者は1人だけ BBCのファーガス・ウォルシュ医療担当編集委員によると、すべての臨床試験は安全性のリスク軽減のために慎重に、ゆっくり開始される。オックスフォード大学で4月に臨床試験が開始された際には、初日に接種を受けたのはボランティア2人だけで、1週間以内に100人に接種された。 これに対して、インペリアル･コレッジ･ロンドンの臨床試験では初日には1人だけにワクチンを接種する。その後48時間ごとに3人に接種し、徐々に被験者を増やしていく。 また、1回分の投与量を使用するオックスフォード大学とは異なり、インペリアル･コレッジ･ロンドンの臨床試験では4週間の間隔をあけて、2回の接種を行うという。 シャトック教授らのチームは、慎重に進めている理由について、ワクチンに特段の安全性の懸念があるからではなく、単にアプローチが新しいからだと説明している。 新型コロナウイルス特集 感染対策 在宅勤務・隔離生活 （英語記事 Human trial of new coronavirus vaccine starts in UK）\n",
      "***** Summary Text (True Value) *****\n",
      "新型コロナウイルスの新しいワクチンの臨床試験がイギリスで始まった。\n",
      "インペリアル・コレッジ・ロンドンのロビン・シャトック教授らが率いる試験の一貫で、今後数週間で約300人を対象に実施される。\n",
      "***** Summary Text (Generated Text) *****\n",
      "英オックスフォード大学の臨床試験で、新型コロナウイルスのワクチンを開発した。\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Input's Text *****\")\n",
    "print(ds[\"test\"][\"text\"][2])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4c854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
