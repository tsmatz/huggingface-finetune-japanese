{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14410483",
   "metadata": {},
   "source": [
    "# Hugging Face - Summarization in Japanese\n",
    "\n",
    "This source code builds the fine-tuned model of [google/mt5-small](https://huggingface.co/google/mt5-small) for Japanese summarization.\n",
    "\n",
    "For more background and details, see [this blog post](https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/).\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/huggingface-finetune-japanese/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4dd94",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "In order to install core components, see [Readme](https://github.com/tsmatz/huggingface-finetune-japanese/).<br>\n",
    "Install additional packages for running this notebook as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bf825",
   "metadata": {},
   "source": [
    "Install packages depending on T5 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a86426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b66d94",
   "metadata": {},
   "source": [
    "Install packages depending on rouge evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee740d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install absl-py rouge_score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a81ff",
   "metadata": {},
   "source": [
    "Install other dependent packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95588a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ef40e",
   "metadata": {},
   "source": [
    "## Check device\n",
    "\n",
    "Check whether GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bc098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled.\n",
      "device count: 1, current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is enabled.\")\n",
    "    print(\"device count: {}, current device: {}\".format(torch.cuda.device_count(), torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"GPU is not enabled.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163ab77",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "In this example, we use [XL-Sum Japanese dataset](https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/japanese) in Hugging Face, which is the annotated article-summary pairs generated by BBC.<br>\n",
    "This dataset has around 7000 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a1866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe42f179a334250a85474fccf0dafd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85358ccff59148659be5a685de280fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xlsum/japanese to /home/tsmatsuz/.cache/huggingface/datasets/csebuetnlp___xlsum/japanese/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b891b2d3342269be44f5a0862abcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xlsum downloaded and prepared to /home/tsmatsuz/.cache/huggingface/datasets/csebuetnlp___xlsum/japanese/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a7310b7f14d73be0e750a4ed53f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 7113\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csebuetnlp/xlsum\", name=\"japanese\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8da181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '44789754',\n",
       " 'url': 'https://www.bbc.com/japanese/44789754',\n",
       " 'title': 'タイ洞窟から少年とコーチ、全員無事救出',\n",
       " 'summary': 'タイ北部のタムルアン洞窟で10日夜、中に閉じ込められていた少年12人とサッカー・コーチの計13人のうち、最後の少年4人とコーチが水路を潜り無事脱出した。その約3時間後には、洞窟内で少年たちと留まっていた海軍ダイバー3人と医師も生還した。17日間も洞窟内にいた13人の救出に、タイ国内外で多くの人が安心し、喜んでいる。',\n",
       " 'text': '救出作戦の間、洞窟内に少年たちと留まったタイ海軍のダイバーと医師も最後に無事脱出した。4人の写真は10日、タイ海軍特殊部隊がフェイスブックに掲載したもの タイ海軍特殊部隊はフェイスブックで、「これは奇跡なのか科学なのか、一体何なのかよくわからない。『イノシシ』13人は全員、洞窟から出た」と救助作戦の終了を報告した。「イノシシ」（タイ語で「ムーパ」）は少年たちの所属するサッカー・チームの愛称。 遠足に出かけた11歳から17歳の少年たちと25歳のサッカー・コーチは6月23日、大雨で増水した洞窟から出られなくなった。タイ内外から集まったダイバー約90人などが捜索に当たり、英国人ダイバー2人によって7月2日夜に発見された。地元のチェンライ県知事やタイ海軍特殊部隊が中心となった救助本部は当初、水が引くか、あるいは少年たちが潜水技術を習得するまで時間をかけて脱出させるつもりだったが、雨季による水位上昇と洞窟内の酸素低下の進行が懸念され、8日から3日連続の救出作戦が敢行された。 少年たちの脱出方法 ダイバーたちに前後を支えられ、水路内に張り巡らされたガイドロープをたどりながら、潜水経験のない少年たちは脱出した。8日に最初の4人、9日に4人、10日に残る5人が脱出し、ただちに近くのチェンライ市内の病院に搬送された。2週間以上洞窟に閉じ込められていたことを思えば、全員驚くほど心身ともに元気だという。 少年たちとコーチはレントゲンや血液検査などを受けた。少なくとも7日間は、経過観察のために入院を続けるという。 洞窟内の水を飲み、鳥やコウモリの排泄物に接触した可能性のある13人は、病原体に感染しているおそれがあるため隔離されている。家族とはガラス越しに再会したという。 食べ物のほとんどない洞窟内で2週間以上を過ごした少年たちは体重を大幅に落とし、空腹を訴えていた。救出後は好物の豚肉のご飯やパン、チョコレートなどを希望したが、しばらくは流動食が続くという。 さらに、外界の光に目が慣れるまでの数日は、サングラスをかける必要がある。 ＜おすすめ記事＞ 救出作戦が終わると、洞窟の出口に集まった救助関係者から大きな歓声が上がった。山のふもとには、少年たちが所属する「ムーパ（イノシシ）」サッカーチームの関係者の家があり、そこに集まった人たちも笑顔で叫んだり歓声を挙げたりした。現場にいたBBCのジョナサン・ヘッド記者は、喜ぶ人たちは「とてもタイ人らしくない様子で」さかんに握手をして回っていたと伝えた。 少年たちへの精神的影響は？ タイ洞窟救助 チェンライ市では、全員脱出の知らせに往来の車は次々にクラクションを鳴らして喜んだ。子供たちやコーチが搬送された病院の外に集まっていた人たちは、一斉に拍手した。 ソーシャルメディアではタイ人の多くが、「#Heroes(英雄）」、「 #Thankyou（ありがとう）」などのハッシュタグを使って、それぞれに思いを表現していた。 13人は2日、洞窟内の岩場に身を寄せているところを発見された。中央の少年は、サッカーのイングランド代表のシャツを着ている。写真はタイ海軍が4日に公表したビデオより サッカー界も少年たちとコーチの無事を大いに喜び、英マンチェスター・ユナイテッドやポルトガルのベンフィカが全員を試合に招待した。国際サッカー連盟（FIFA）も、少年たちをロシアで開催されているワールドカップの15日にある決勝戦に招いたが、これは回復が間に合わないという理由で見送られた。 ワールドカップの準決勝に備えるイングランド代表のDFカイル・ウォーカーは、イングランドのユニフォームを少年たちに贈りたいとツイートした。少年の1人は洞窟内で、イングランドのジャージーを着ていた。すると英外務省の公式アカウントがこれに応えて、「やあ、カイル。駐タイ英国大使と話をした。イングランドのシャツを勇敢な少年たちに、喜んで、確実に届けてくれるそうだ」とツイートした。 経験豊富なダイバーにとっても、少年たちのいる場所までの往復は重労働だった。元タイ海軍潜水士のサマン・グナンさんは6日、少年たちに空気ボンベを運ぶ任務を果たして戻ろうとしていたところ、酸素不足で命を落とした。 ダイバーたちが出口まで張ったガイドロープをたどりながら、少年たちは場所によって、歩いたり、水の中を歩いたり、登ったり潜ったりして外に出た。 少年たちは、通常のマスクよりも初心者に適した顔部全体を覆うマスクをかぶった。少年1人につき2人のダイバーが付き、ダイバーが少年の空気ボンベを運んだ。 最も困難なのは、洞窟の中ほどにある「Tジャンクション」と呼ばれている場所で、あまりに狭いため、ダイバーは空気ボンベを外して進む必要があった。 Tジャンクションを抜けると、ダイバー達の基地となっている「第3室」があり、少年たちはここで出口へ向かう前に休息がとれた。 少年らの救出経路。下方の赤い丸が少年たちの見つかった場所。人の形が実際の人間の身長。青い部分は潜水しないと進めない。高さが1メートルに満たない箇所もある。トンネル内で最も狭い部分は、人1人がやっと通れるぐらいのスペースしかない。上方の白い部分は、ところどころ浅い水があるが、ほとんどが乾いた岩場 （英語記事 Cave rescue: Elation as Thai boys and coach freed by divers）'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c922d",
   "metadata": {},
   "source": [
    "To generate inputs for fine-tuning, now I tokenize each text and convert into token ids.\n",
    "\n",
    "First, load tokenizer in pre-trained ```google/mt5-small``` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42045d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1f8f07684d4539b5b5dfb4596e1db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6756bdc7074c4350a03ec8dc7184eea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adf5b062bb348169ee1034997764ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86b5fd2d70a49ed9347d0846582aad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8231f7",
   "metadata": {},
   "source": [
    "For fine-tuning, apply tokenization for dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714d1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample_data(data):\n",
    "    # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "    # I then restrict these token size.\n",
    "    input_feature = t5_tokenizer(data[\"text\"], truncation=True, max_length=1024)\n",
    "    label = t5_tokenizer(data[\"summary\"], truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": input_feature[\"input_ids\"],\n",
    "        \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "        \"labels\": label[\"input_ids\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b32615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f93732f5f0943e1bd532a7198ae0e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21ca276ea984295a57730ea74d03700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c48ec483b541c18776183f0a8d6387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7113\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 889\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(\n",
    "    tokenize_sample_data,\n",
    "    remove_columns=[\"id\", \"url\", \"title\", \"summary\", \"text\"],\n",
    "    batched=True,\n",
    "    batch_size=128)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dd056",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf379a1",
   "metadata": {},
   "source": [
    "In this example, we use mT5 model.<br>\n",
    "There exist several sizes of mT5 and I'll use small one (```google/mt5-small```) to fit to memory in my machine. The name is \"small\", but it's still so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cab1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e757d77c000493a8939e4206ffd7edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "# see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "mt5_config = AutoConfig.from_pretrained(\n",
    "    \"google/mt5-small\",\n",
    "    max_length=128,\n",
    "    length_penalty=0.6,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=15,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb5336",
   "metadata": {},
   "source": [
    "We prepare data collator, which works for preprocessing data.\n",
    "\n",
    "For the sequence-to-sequence (seq2seq) task, we need to not only stack the inputs for encoder, but also prepare for the decoder side. In seq2seq setup, a common technique called \"teach forcing\" will then be applied in decoder.<br>\n",
    "These tasks are not needed to manually setup in Hugging Face, and ```DataCollatorForSeq2Seq``` will take care of all steps.\n",
    "\n",
    "In this collator, the padded token will also be filled with label id -100.<br>\n",
    "This token will then be ignored in the sebsequent loss computation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc213f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    t5_tokenizer,\n",
    "    model=model,\n",
    "    return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c9f14",
   "metadata": {},
   "source": [
    "We also prepare metrics function for evaluation in the training.<br>\n",
    "Measuring the quality of generated text is very difficult, and BLEU and ROUGE are often used.\n",
    "\n",
    "Briefly speaking, BLEU measures how many of n-grams in the generated (predicted) text are overlaped in the reference text. This score is used for evaluation, especially in the machine translation task.\n",
    "However, in summarization, we need all important words (which appears on the reference text) in the generated text. This is because we often use ROUGE in summarization tasks.\n",
    "The idea of ROUGE is similar to BLEU, but it also measures how many of n-grams in the reference text appears in the generated (predicted) text. (This is why the name of ROUGE includes \"RO\", which means \"Recall-Oriented\".)<br>\n",
    "There also exist variations, ROUGE-L and ROUGE-Lsum, which also measures the longest common substrings (LCS).\n",
    "\n",
    "In Hugging Face, you don't need to manually implement these logics and can use built-in objects for scoring these matrics.<br>\n",
    "In this example, I have configured mT5 tokenization as custom tokenization in computation (which is based on SentencePiece Unigram segmentation), because the white space tokenization is used as default in ROUGE evaluation.\n",
    "\n",
    "> Note : You can also specify multilingual stemmer.\n",
    "\n",
    "> Note : As I have mentioned above, the padded token id becomes -100 by data collator and I then also convert it into padded token id before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5dd0da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91d840aa5a04f33abd2829524219220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg = t5_tokenizer(arg)\n",
    "    return t5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Add punctuation before sentence tokenization\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde59f4",
   "metadata": {},
   "source": [
    "Before fine-tuning, now I check ROUGE score with plain mT5 model. Here I check scores for top 5 rows in test dataset.\n",
    "\n",
    "The score is very low, because this model is not trained for any downstream tasks. (It's just trained by unsupervised approach.)\n",
    "\n",
    "> Note : In order to avoid suboptimal text generation, here I have applied beam search for the text generation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e99037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.0984516978372496,\n",
       " 'rouge2': 0.031058885487640298,\n",
       " 'rougeL': 0.09167203682030044,\n",
       " 'rougeLsum': 0.09731889552617845}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "    tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "    with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            num_beams=15,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=1,\n",
    "            remove_invalid_values=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "    break\n",
    "\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bfded",
   "metadata": {},
   "source": [
    "We prepare training arguments for fine-tuning.<br>\n",
    "In this example, we use HuggingFace transformer trainer class, with which you can run training without manually writing training loop.\n",
    "\n",
    "In usual training evaluation, training loss and accuracy will be computed and evaluated, by comparing the generated logits with labels. However, as we saw above, we want to evaluate ROUGE score using the predicted tokens.<br>\n",
    "To simplify these sequence-to-sequence specific steps, here I use built-in ```Seq2SeqTrainingArguments``` and ```Seq2SeqTrainer``` classes in HuggingFace, instead of usual ```TrainingArguments``` and ```Trainer```.<br>\n",
    "By setting ```predict_with_generate=True``` in this class, the predicted tokens generated by  ```model.generate()``` will be used in each evaluation.\n",
    "\n",
    "The checkpoint files (in each 500 steps) are saved in the folder named ```mt5-summarize-ja```.\n",
    "\n",
    "> Note : Do not use FP16 precision in mT5 fine-tuning.\n",
    "\n",
    "> Note : In general, the saved checkpoints in the training will become so large.<br>\n",
    "> Set ```save_total_limit``` property (which limits the total amount of checkpoints by deleting the older ones) to save disks, or expand disks in Azure VM. (See [here](https://learn.microsoft.com/en-us/azure/virtual-machines/linux/expand-disks) to expand disks in Azure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f28731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"mt5-summarize-ja\",\n",
    "    log_level = \"error\",\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-4,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    warmup_steps = 90,\n",
    "    optim = \"adafactor\",\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length = 128,\n",
    "    save_steps = 500,\n",
    "    logging_steps = 10,\n",
    "    push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978f071",
   "metadata": {},
   "source": [
    "Build trainer. (Put it all together.)\n",
    "\n",
    "Because the cost of evaluation computation (ROUGE scoring) is so high, I have then decreased the number of rows in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca8a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = metrics_func,\n",
    "    train_dataset = tokenized_ds[\"train\"],\n",
    "    eval_dataset = tokenized_ds[\"validation\"].shard(num_shards=45, index=0),\n",
    "    tokenizer = t5_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b530f",
   "metadata": {},
   "source": [
    "Now let's run training.\n",
    "\n",
    "As I have mentioned above, make sure that you have enough disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66059ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4440' max='4440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4440/4440 6:28:46, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.708100</td>\n",
       "      <td>3.617864</td>\n",
       "      <td>0.218290</td>\n",
       "      <td>0.085383</td>\n",
       "      <td>0.176849</td>\n",
       "      <td>0.189304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.126300</td>\n",
       "      <td>3.318559</td>\n",
       "      <td>0.253556</td>\n",
       "      <td>0.114019</td>\n",
       "      <td>0.214312</td>\n",
       "      <td>0.224454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.981500</td>\n",
       "      <td>3.255641</td>\n",
       "      <td>0.263754</td>\n",
       "      <td>0.124873</td>\n",
       "      <td>0.226179</td>\n",
       "      <td>0.230825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.840400</td>\n",
       "      <td>3.177551</td>\n",
       "      <td>0.267794</td>\n",
       "      <td>0.128709</td>\n",
       "      <td>0.228947</td>\n",
       "      <td>0.235092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.861000</td>\n",
       "      <td>3.105559</td>\n",
       "      <td>0.287856</td>\n",
       "      <td>0.141241</td>\n",
       "      <td>0.237337</td>\n",
       "      <td>0.246780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.724500</td>\n",
       "      <td>3.073758</td>\n",
       "      <td>0.303298</td>\n",
       "      <td>0.148474</td>\n",
       "      <td>0.246282</td>\n",
       "      <td>0.259047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.533500</td>\n",
       "      <td>3.049917</td>\n",
       "      <td>0.298712</td>\n",
       "      <td>0.143530</td>\n",
       "      <td>0.238193</td>\n",
       "      <td>0.251933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.667000</td>\n",
       "      <td>3.008084</td>\n",
       "      <td>0.287892</td>\n",
       "      <td>0.139529</td>\n",
       "      <td>0.232503</td>\n",
       "      <td>0.245836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.603500</td>\n",
       "      <td>2.990826</td>\n",
       "      <td>0.276351</td>\n",
       "      <td>0.144607</td>\n",
       "      <td>0.233419</td>\n",
       "      <td>0.244171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.568800</td>\n",
       "      <td>2.980506</td>\n",
       "      <td>0.295757</td>\n",
       "      <td>0.139312</td>\n",
       "      <td>0.238046</td>\n",
       "      <td>0.251453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.398000</td>\n",
       "      <td>2.962016</td>\n",
       "      <td>0.298322</td>\n",
       "      <td>0.142058</td>\n",
       "      <td>0.239235</td>\n",
       "      <td>0.253993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.415100</td>\n",
       "      <td>2.925102</td>\n",
       "      <td>0.286023</td>\n",
       "      <td>0.134510</td>\n",
       "      <td>0.228799</td>\n",
       "      <td>0.244077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.509200</td>\n",
       "      <td>2.931422</td>\n",
       "      <td>0.288727</td>\n",
       "      <td>0.143593</td>\n",
       "      <td>0.240820</td>\n",
       "      <td>0.253648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.442300</td>\n",
       "      <td>2.908769</td>\n",
       "      <td>0.302843</td>\n",
       "      <td>0.141274</td>\n",
       "      <td>0.246203</td>\n",
       "      <td>0.260172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.414200</td>\n",
       "      <td>2.903538</td>\n",
       "      <td>0.306332</td>\n",
       "      <td>0.155801</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>0.263840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.391800</td>\n",
       "      <td>2.894337</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.156808</td>\n",
       "      <td>0.253540</td>\n",
       "      <td>0.265641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.339900</td>\n",
       "      <td>2.883549</td>\n",
       "      <td>0.296630</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.249597</td>\n",
       "      <td>0.261012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.356300</td>\n",
       "      <td>2.880027</td>\n",
       "      <td>0.332355</td>\n",
       "      <td>0.161110</td>\n",
       "      <td>0.264872</td>\n",
       "      <td>0.276773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.304000</td>\n",
       "      <td>2.872157</td>\n",
       "      <td>0.331778</td>\n",
       "      <td>0.157280</td>\n",
       "      <td>0.260618</td>\n",
       "      <td>0.272548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.453400</td>\n",
       "      <td>2.866482</td>\n",
       "      <td>0.320811</td>\n",
       "      <td>0.153777</td>\n",
       "      <td>0.257344</td>\n",
       "      <td>0.269723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.359700</td>\n",
       "      <td>2.869891</td>\n",
       "      <td>0.321716</td>\n",
       "      <td>0.158310</td>\n",
       "      <td>0.255834</td>\n",
       "      <td>0.273247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.298800</td>\n",
       "      <td>2.868351</td>\n",
       "      <td>0.328945</td>\n",
       "      <td>0.155585</td>\n",
       "      <td>0.258368</td>\n",
       "      <td>0.270259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4440, training_loss=3.722657856640515, metrics={'train_runtime': 23332.2415, 'train_samples_per_second': 3.049, 'train_steps_per_second': 0.19, 'total_flos': 6.391027359108096e+16, 'train_loss': 3.722657856640515, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44578fb6",
   "metadata": {},
   "source": [
    "In order to use it later, you can save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0710012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./trained_for_summarization_jp\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "    trainer.model.module.save_pretrained(\"./trained_for_summarization_jp\")\n",
    "else:\n",
    "    trainer.model.save_pretrained(\"./trained_for_summarization_jp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2efe66",
   "metadata": {},
   "source": [
    "Load pre-trained model from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be7c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"./trained_for_summarization_jp\")\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6f566",
   "metadata": {},
   "source": [
    "## Summarize with Fine-Tuned Model\n",
    "\n",
    "Now let's see how it generates text for summarization with fine-tuned model.<br>\n",
    "Here I generate the summarized text of test data, which has not seen in the training set.\n",
    "\n",
    "> Note : You can also use ```predict()``` method in Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca11263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "トム・エッジントン BBCリアリティー・チェック（ファクトチェック）チーム かつて労働党党首も務めたブレア氏は、BBCラジオ4の番組「Today」で、「議会は行き詰まった。議会が決められないなら、国民が決める形に戻ろう」と語った。 労働党の公式な立場は、テリーザ・メイ英首相が欧州連合（EU）と合意した離脱協定案が議会で否決された場合、解散総選挙の圧力をかけるというもの。もし総選挙が実現しなかった場合は、再度の国民投票を支持するのも選択肢になりうると、労働党は表明している。 しかしメイ首相は、再国民投票の予測を否定している。メイ氏は下院議員らに対し、2016年に実施した国民投票の結果が「尊重されるべきだ」と繰り返し語ってきた。 だが、もしブレア氏が求めている通り、下院がブレグジットをめぐる膠着（こうちゃく）状態を打ち破るために2度目の国民投票を実施すると決定したら、どうなるのだろうか？ 英選挙管理委員会はBBCニュースに対し、「適切な対応策」を有しており、「あらゆる予定外の投票に迅速に対応する」準備ができていると語った。 期限は迫っている イギリスのEU離脱予定日は、2019年3月29日。残り100日を切り、時間が最も差し迫った問題だ。 英議会が2度目の国民投票実施を採択した場合、投票規則や選挙運動規則を定める法律にも上下両院の支持が必要になる。 2016年の国民投票では、投票日の7カ月前に関連法案が議決された。 しかし、今回はもっと早い法制化が可能なのだろうか？ 法制化の速度を上げるため、前回の国民投票に関する諸規則を定めた2015年国民投票法をひな型にし、実質的に大部分を写してしまうのが、あり得る選択肢の1つだ。 英ユニヴァーシティー・コレッジ・ロンドン公共政策大学院憲法ユニットのアラン・レンウィック副ユニット長は、「理論上、このやり方は非常に素早く完了できる」と話す。 もしこのやり方が採用されても、法案の議会通過はおよそ11週間かかるとレンウィック氏は推計している。 この予定表を基にすると、法案通過は2月後半になると予想される。ただし、法制過程を今開始すればの話だ。 投票用紙の選択肢を、2016年の国民投票における「離脱」か「残留」かの2択ではなくし、複数の選択肢を含めるよう下院が要求した場合、かかる時間はもっとずっと長くなると、レンウィック氏は付け加える。 2016年の国民投票でEU残留派として活動したトニー・ブレア元首相は、2度目の国民投票が議会の膠着状態を解消する可能性があると主張する 何を問うのか あり得る選択肢の範囲からどんな質問を選ぶかは、最終的には英議会の決定に委ねられる。 離脱協定の最終合意に国民投票を求める「People's Vote （人民の投票）」運動の見解では、テリーザ・メイ首相の離脱協定とEU残留のどちらかを選ぶのが推奨される選択肢だが、投票参加者に3つの選択肢から選ばせる可能性も除外しないという。 再び国民投票が実施されるなら、投票用紙に「残留」の選択肢はあるべきでなく、メイ首相の離脱協定か、合意なしでのEU離脱かの二者択一であるべきとの主張もある。 他の選択肢もある。デイヴィッド・キャメロン内閣で運輸相や国際開発相、メイ内閣で教育相や女性・平等担当相を歴任し、2度目の国民投票を支持しているジャスティーン・グリーニング氏は以前、3つの選択肢を求めた――。 複数の選択肢がある場合、下院はどんな投票制度を使うかも決定する必要がある。たとえば、選択肢を1つ選ぶのか、望ましいほうから順番をつけるのか、というように。 選挙管理委員会は、提案された質問を試験し、それらが「明白に、単純に、そして中立に」示されていると確認する必要もある。 選挙運動を行う公認団体の選定も必要だ。 選挙管理委員会はそれから、国民投票の参加方法を有権者に情報提供する必要がある。また、全国で開票担当者の確保も必要だ。 これらの準備が終わると、選挙運動期間が始まる。運動期間は、通常4週間続く。そしてやっと、投票そのものが実施される。 ジャスティーン・グリーニング氏は2度目の国民投票案を支持し、国民には3つの選択肢が与えられるべきだと主張する 選管はBBCニュースに対し、2000年制定の政党、選挙、国民投票法で定められている法案通過から投票当日までの全ての手順には、最短でも10週間かかると説明した。 このことから、法案通過と選挙過程の両方が、イギリスがEUを離脱する予定期日である2019年3月29日までに終わる可能性は極めて低いと示唆される。 発表から10日での国民投票 しかし、非常に厳しい時期内に国民投票を実施した前例が他国にないわけではない。 3年前、ギリシャは1週間ほどの準備期間で国民投票をとりまとめた。有権者はこの国民投票で、同国の経済危機に対する国際債権団の救済案を否決した。 しかし、国民投票をあまりに早急に実施してしまうと、「通常の手続きに従っていない」との印象を与え、有権者が最終結果を非合法なものとみてしまう可能性があると、レンウィック氏は語る。 たとえば、2015年のギリシャと似た準備期間で国民投票をすると、郵送による投票を受け付けたり、投票用紙に書かれる質問を評価したりする十分な時間の確保が許されないことになる。 リスボン条約50条で定められた期間の延長 イギリスはEUに対し、EU基本条約（リスボン条約）第50条で定められた離脱交渉期間を延長するよう求める可能性もある。リスボン条約は、メイ首相が第50条を発動した2017年3月29日から2年間を、離脱条件の合意に必要な期間として定めている。この期間が延長されれば、新たな国民投票を実施するための時間が増える。 しかし、英ケンブリッジ大学で欧州法を研究するキャスリン・バーナード教授によると、2019年3月29日という離脱期限を延長するあらゆる試案には、他のEU加盟27カ国の全会一致での支持が必要になる。 EUは、イギリスの離脱延期を認める可能性があると示唆している。ただし、たとえば総選挙もしくは新たな国民投票が実施されるなど、政局が変化した場合のみだという。すでに合意された離脱協定について、単に再交渉するための追加時間の確保は認められない。 また、期間延長にはイギリス議会の同意も必要になる。 ＜関連記事＞ さらに欧州司法裁判所（ECJ）は先に、イギリスは他国の承認を得ずにリスボン条約第50条の発動を完全に取り消す権限を持つとの判断を下した。 しかしこれは、ブレグジットの過程全体を中止できるという意味だ。単に延期するという意味ではない。 なので結局、イギリスが2度目の国民投票実施を望むなら、まずは第50条で定められた離脱期間の延長を模索することになるだろう。 そして、投票の結果に従って、イギリスは投票後に第50条の発動を撤回するかどうか決められる。 イギリスのEU離脱予定日後に国民投票を実施するのも代替案かもしれない。しかし、この案はかなり現実的な困難を引き起こしかねない。特に、既に離脱したにもかかわらず、イギリス国民がEUの一員であり続ける選択をした場合には。 （英語記事 Brexit: How could another referendum on leaving the EU work?）\n",
      "***** Summary Text (True Value) *****\n",
      "ブレグジット(イギリスの欧州連合離脱)をめぐり、イギリスのトニー・ブレア元首相が、2度目の国民投票を求める主張を再び繰り広げている。可能性があるブレグジットの形態1つ1つについて議会で投票し、どれも合意に達しない場合に実施すべきという。\n",
      "***** Summary Text (Generated Text) *****\n",
      "イギリスの欧州連合(EU)離脱について、英議会は2度目の国民投票を実施すると発表した。\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "    tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "    with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            num_beams=15,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=1,\n",
    "            remove_invalid_values=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "    break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(ds[\"test\"][\"text\"][0])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[0])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22c55eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "イングランドWTBメイは2分間で2つのトライを決めた 前回大会で1次リーグ敗退の屈辱を味わったイングランドにとっては、3大会ぶりの準決勝進出。 26日に横浜で開かれる準決勝で、大会3連覇を狙う世界王者ニュージーランドと対戦する。ニュージーランドはこの日、準々決勝の2試合目でアイルランドを破って4強入りした。 イングランドは3勝無敗（1試合は雨天引き分け）で1次リーグC組を1位突破。一方、オーストラリアは、D組を3勝1敗で2位通過していた。 イングランドは1次リーグ最終戦が台風の影響で中止となり、5日以来2週間ぶりの試合だった。たっぷりと休養を取った一方、すぐに本来の動きを発揮できるのか不安視する声もあったが、無用の心配だった。 トライですぐ逆転 先制点はオーストラリアが挙げた。 前半11分、イングランドが危険なハイタックルの反則を犯すと、オーストラリアのSOクリスチャン・リアリーファノがペナルティゴール決めた。 しかし、イングランドの反撃は早かった。 前半17分、イングランドは右サイドから左サイドへと大きくパスをつなぎ、最後はWTBジョニー・メイが左サイドに飛びこんで逆転。SOオウエン・ファレルがコンバージョンキックを決めた。 その3分後、メイが再びトライを決める。オーストラリアのパスをインターセプトしたCTBヘンリー・スレイドが駆け上がり、前方にゴロのキックを蹴り出した。それをメイがつかみ、またも左サイドに滑り込んだ。 コンバージョンキックも決まり、イングランドは14－3とリードを広げた。この日、キッカーのファレルは抜群の安定性を見せた。 トライ狙わず確実に得点 前半25分、イングランドは自陣ゴールから10メートル足らずの場所で反則を犯す。オーストラリアはこの好機に、迷わずペナルティキックを選択。トライに固執せず着実に点差を詰める、決勝トーナメントらしい戦術をとった。 これをリアリーファノが確実に決め、6－14に点差を縮めた。 イングランドは前半29分、ファレルが相手反則から約30メートルのペナルティゴールを成功させた。 しかし前半終了間際、オーストラリアのリアリーファノもペナルティゴールを決め返し、9－17の8点差でハーフタイムを迎えた。 猛追を予感させたが 1次リーグの試合では後半に得点を集中させ、スロースターターぶりを見せたオーストラリアは、この日も後半、猛追を予感させる見事な動きから再始動した。 後半2分、WTBマリカ・コロイベティが見事なスピードとステップでディフェンスをかわすと、一気にゴールエリアまで駆け込んだ。 コンバージョンキックも成功。1点差に詰め寄った。 しかし、イングランドは落ち着きを失わず、自分たちのペースを乱さなかった。 イングランドのPRシンクラーのトライは、反撃ムードのオーストラリアにとって痛手となった 後半5分、パスを受けたPRカイル・シンクラーが相手ディフェンスラインのすき間を突破し、ゴール中央部分にトライ。コンバージョンキックも決め、再び8点差に戻した。 後半10分にはファレルがゴールポスト正面からペナルティゴールに成功。リードを27－16に広げた。 勝負決めた攻防 一方、オーストラリアは後半18分、ゴール目前のマイボールのスクラムから波状攻撃を展開。フォワード陣の突進でゴール2メートルまで迫る場面もあったが、イングランドは体を張って押し戻し続け、ついにはボールを奪うことに成功した。 オーストラリアにとっては大きなチャンスを逃した場面だった。これで気落ちしたのか、オーストラリアは以降、見せ場をほとんど作れなかった。 反対にイングランドは、後半25分と33分に、ファレルがこの試合3つ目と４つ目のペナルティゴールをともに成功させ、オーストラリアを突き放した。 後半35分には、オーストラリアが左に放った長いパスをイングランドのWTBアントニー・ワトソンがインターセプトし、ゴールまで駆け上がってダメ押しのトライを決めた。 オーストラリアは後半37分、コロイベティが再び快足を飛ばし、ディフェンスを振り切ってゴールエリアまで駆け込んだ。しかし、その前のプレーでパスがスローフォワードの反則と判断され、トライは無効になった。 直後、試合終了の鐘が鳴った。 ＜関連記事＞ イングランドのエディ・ジョウンズ監督は試合後、「最初の20分間は相手にボールを75％支配されていたが、選手たちは見事に粘った。うまく守り、流れを取り戻した」と選手たちを称えた。 さらに、「後半、相手が反撃してきて、『かかってこい』となったが、うまく対応できた」と振り返った。 「準決勝進出に、みんなすごく盛り上がっている。まだ最高潮になっていないので、その状態にどうやってたどりつくかが課題だ」 21歳のイングランドのFLカリーは終始素晴らしい動きを見せ続けた この試合の最優秀選手：トム・カリー（イングランド） この試合の最優秀選手には、16回のタックルをするなど、終始見事なプレーを見せたイングランドのFLトム・カリーが選ばれた。 （英語関連記事 England beat Australia to make semis）\n",
      "***** Summary Text (True Value) *****\n",
      "ラグビーワールドカップ(W杯)日本大会は19日、決勝トーナメントが始まり、大分スポーツ公園総合競技場(大分市)であった準々決勝の第1試合ではイングランド(世界3位)が40-16でオーストラリア(同6位)を下し、ベスト4に一番乗りを決めた。\n",
      "***** Summary Text (Generated Text) *****\n",
      "オーストラリアは26日、準々決勝進出を狙う世界王者ニュージーランドと対戦した。\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Input's Text *****\")\n",
    "print(ds[\"test\"][\"text\"][1])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[1])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e8ae2",
   "metadata": {},
   "source": [
    "Below shows ROUGE scores for our fine-tuned model.<br>\n",
    "You will find that it has high scores rather than previous results which is not fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fb4f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.40136296616652434,\n",
       " 'rouge2': 0.21677226048085055,\n",
       " 'rougeL': 0.33508386350491615,\n",
       " 'rougeLsum': 0.35111111111111115}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add punctuation before sentence tokenization\n",
    "text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "\n",
    "# Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "\n",
    "# compute ROUGE score with custom tokenization\n",
    "rouge_metric.compute(\n",
    "    predictions=text_preds,\n",
    "    references=text_labels,\n",
    "    tokenizer=tokenize_sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4c854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
